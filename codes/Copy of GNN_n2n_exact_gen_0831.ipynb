{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of GNN_n2n_exact_gen_0831_copy.ipynb","provenance":[{"file_id":"1yH55jcqo2EZ9wVWSiPdFfxyqbMTDEEpC","timestamp":1599494521560}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"dJ1P8LLwoCB-","colab_type":"text"},"source":["# GNN for Optimal Generation Dispatch\n","## IEEE 24-bus RTS system \n","* System settings: 24 buses, 34 lines (concatenated from 38), 17 loads, 12 generating units on 10 buses.\n","* Data: Currently we use a dataset with ~ 6000 samples, 80% used for training and 20% for validation."]},{"cell_type":"markdown","metadata":{"id":"eOjKm_RBoCCA","colab_type":"text"},"source":["Step 1: Creating a graph in DGL\n","-------------------------------\n","* Load necessary packages.\n","* Read the data and create the graph for the system:\n"]},{"cell_type":"code","metadata":{"id":"SM9Zk1p9oOXU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":255},"executionInfo":{"status":"ok","timestamp":1599494698291,"user_tz":300,"elapsed":4140,"user":{"displayName":"Jeehyun Park","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GizNw5DW2QVCa5vUMiz0MiTNIYXIkGzaDCp0Es2=s64","userId":"14458427846939093236"}},"outputId":"c377e006-550b-4545-90be-56024d80e6d3"},"source":["!pip install dgl"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting dgl\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/09/92974857a7574431a81a9a3ef0ee02b55f30540881762e59e68886ca4221/dgl-0.5.1-cp36-cp36m-manylinux1_x86_64.whl (3.5MB)\n","\u001b[K     |████████████████████████████████| 3.5MB 5.7MB/s \n","\u001b[?25hRequirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.6/dist-packages (from dgl) (2.5)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from dgl) (2.23.0)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from dgl) (1.18.5)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from dgl) (1.4.1)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.1->dgl) (4.4.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->dgl) (2020.6.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->dgl) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->dgl) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->dgl) (3.0.4)\n","Installing collected packages: dgl\n","Successfully installed dgl-0.5.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6Ozf8r2noil-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1599495105808,"user_tz":300,"elapsed":40203,"user":{"displayName":"Jeehyun Park","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GizNw5DW2QVCa5vUMiz0MiTNIYXIkGzaDCp0Es2=s64","userId":"14458427846939093236"}},"outputId":"165ec95b-bab7-4b23-cb56-067be9005aaa"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JKLovDeXoCCP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1599495119190,"user_tz":300,"elapsed":7086,"user":{"displayName":"Jeehyun Park","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GizNw5DW2QVCa5vUMiz0MiTNIYXIkGzaDCp0Es2=s64","userId":"14458427846939093236"}},"outputId":"ad29f06e-0a41-49a5-f9d0-b67373f5f8b6"},"source":["# Import necessary packages\n","import dgl # graph convolution\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"],"execution_count":9,"outputs":[{"output_type":"stream","text":["DGL backend not selected or invalid.  Assuming PyTorch for now.\n","Using backend: pytorch\n"],"name":"stderr"},{"output_type":"stream","text":["Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RamArMRLoCC7","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599495155465,"user_tz":300,"elapsed":2753,"user":{"displayName":"Jeehyun Park","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GizNw5DW2QVCa5vUMiz0MiTNIYXIkGzaDCp0Es2=s64","userId":"14458427846939093236"}}},"source":["# load the data files\n","filename = './drive/My Drive/Colab Notebooks/ieee24_rts_6183exact_0831.txt' # active level 0.1024(line0.0124), max active line 3\n","data = pd.read_table(filename,sep=',',header=None).to_numpy()\n","\n","# system size\n","n_bus  = int(data[0,0].copy())\n","n_line  = int(data[1,0].copy())\n","n_load  = int(data[2,0].copy())\n","n_sample  = int(data[3,0].copy())\n","\n","# line connection\n","line_bus = data[:,1:3].copy()\n","\n","# load and corresponding line data\n","load_data0 = data[:,3:n_sample+3].copy()\n","line_flow = data[:,n_sample + 3 : 2*n_sample + 3].copy()\n","bound_idx_low = data[:,2*n_sample + 3 : 3*n_sample + 3].copy()\n","bound_idx_up = data[:,3*n_sample + 3 : 4*n_sample + 3].copy()\n","bound_idx0 = bound_idx_up + bound_idx_low # may use 'np.add(a,-b)' instead\n","line_limit = data[:,4*n_sample + 3].copy()\n","# print(line_limit)\n","\n","# generation data\n","gen_exact = data[:,4*n_sample + 4 : 5*n_sample + 4].copy()\n","gen_low_limit = data[:,5*n_sample + 4].copy()\n","gen_up_limit = data[:,5*n_sample + 5].copy()\n","\n","# normalize the generation data\n","y0 = np.zeros((n_bus,n_sample))\n","for i in range(n_sample):\n","  # y0[:,i] = np.divide(line_flow[:,i],line_limit) # w/ direction\n","  for j in range(n_bus):\n","    if gen_up_limit[j] > 0:\n","      y0[j,i] = np.divide(gen_exact[j,i]-gen_low_limit[j],gen_up_limit[j]-gen_low_limit[j])\n","    else:\n","      y0[j,i] = 0\n","\n","  \n","\n","# % exact generation\n","# data1(1:N,4*n+5:5*n+4) = gen_data;\n","# % generation lower bound & upper bound\n","# data1(1:N,5*n+5) = g_min;\n","# data1(1:N,5*n+6) = g_max;\n"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"WzgofvUMT1G-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1599495157503,"user_tz":300,"elapsed":318,"user":{"displayName":"Jeehyun Park","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GizNw5DW2QVCa5vUMiz0MiTNIYXIkGzaDCp0Es2=s64","userId":"14458427846939093236"}},"outputId":"7dc1636c-d514-45a5-d81e-3e21a9b8ee18"},"source":["print(np.max(y0)) # check the normalization\n","print(np.min(y0))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["1.0\n","0.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yfMmqGJmru9O","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1599495159819,"user_tz":300,"elapsed":461,"user":{"displayName":"Jeehyun Park","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GizNw5DW2QVCa5vUMiz0MiTNIYXIkGzaDCp0Es2=s64","userId":"14458427846939093236"}},"outputId":"1470369a-dc8d-4108-f259-a6ffa985b231"},"source":["# Use partial data for faster test\n","n_sample = 6000\n","\n","load_data = load_data0[:,:n_sample].copy()\n","bound_idx = bound_idx0[:,:n_sample].copy()*1\n","\n","# gen_idx_low = gen_idx_low[:,:n_sample]\n","# gen_idx_up = gen_idx_up[:,:n_sample]\n","# gen_idx = gen_idx_up + gen_idx_low\n","\n","# print system info\n","print('Test case: ',filename)\n","print('Number of buses: ',n_bus)\n","print('Number of lines: ',n_line)\n","print('Number of loads: ',n_load)\n","print('Number of samples: ',n_sample)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Test case:  ./drive/My Drive/Colab Notebooks/ieee24_rts_6183exact_0831.txt\n","Number of buses:  24\n","Number of lines:  34\n","Number of loads:  17\n","Number of samples:  6000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mf9YAQgToCEO","colab_type":"text"},"source":["* Generate and visualize the DGL graph"]},{"cell_type":"code","metadata":{"id":"NxaGvjWGoCEH","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599495162760,"user_tz":300,"elapsed":708,"user":{"displayName":"Jeehyun Park","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GizNw5DW2QVCa5vUMiz0MiTNIYXIkGzaDCp0Es2=s64","userId":"14458427846939093236"}}},"source":["# Graph generating function\n","def build_system_graph(src,dst):\n","    # Edges are directional in DGL; Make them bi-directional.\n","    # Matlab counts from 1 and python from 0\n","    u = np.concatenate([src, dst])-1 \n","    v = np.concatenate([dst, src])-1\n","    # Construct a DGLGraph\n","    return dgl.DGLGraph((u, v))"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"H3zws2wBoCEd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1599495163662,"user_tz":300,"elapsed":365,"user":{"displayName":"Jeehyun Park","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GizNw5DW2QVCa5vUMiz0MiTNIYXIkGzaDCp0Es2=s64","userId":"14458427846939093236"}},"outputId":"7710c6dc-932e-40f1-b648-eca13c837273"},"source":["line_src = line_bus[:,0].copy().astype(int)\n","line_dst = line_bus[:,1].copy().astype(int)\n","G = build_system_graph(line_src,line_dst)\n","print('There are %d nodes.' % G.number_of_nodes())\n","print('There are %d edges.' % G.number_of_edges())\n","\n","import networkx as nx\n","# Since the actual graph is undirected, we convert it for visualization\n","# purpose.\n","# nx_G = G.to_networkx().to_undirected()\n","# # Kamada-Kawaii layout usually looks pretty for arbitrary graphs\n","# pos = nx.kamada_kawai_layout(nx_G)\n","# nx.draw(nx_G, pos, with_labels=True, node_color=[[.7, .7, .7]])"],"execution_count":15,"outputs":[{"output_type":"stream","text":["There are 24 nodes.\n","There are 68 edges.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/dgl/base.py:45: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n","  return warnings.warn(message, category=category, stacklevel=1)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"HLmJ64bCoCFJ","colab_type":"text"},"source":["Step 2: Assign features to nodes, edges\n","--------------------------------------------\n","e.g.: Graph neural networks associate features with nodes and edges for training.\n","\n","* In our case, inputs on nodes, features on egdes\n"]},{"cell_type":"code","metadata":{"id":"7xqplmGwoCFe","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599495166537,"user_tz":300,"elapsed":308,"user":{"displayName":"Jeehyun Park","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GizNw5DW2QVCa5vUMiz0MiTNIYXIkGzaDCp0Es2=s64","userId":"14458427846939093236"}}},"source":["# # In DGL, you can add features for all nodes at once, using a feature tensor that\n","# # batches node features along the first dimension. The code below adds the learnable\n","# # embeddings for all nodes:\n","\n","# embed_feature = 10\n","# embed = nn.Embedding(G.number_of_edges(), embed_feature)  \n","# # 68 edges with embedding dim equal to 10 for edge classification\n","# G.edata['feat'] = embed.weight"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eHOzg-9PoCF3","colab_type":"text"},"source":["Step 3: Define a Graph Convolutional Network (GCN) and Train the model\n","--------------------------------------------------\n","To perform node classification, use the Graph Convolutional Network\n","(GCN) developed by `Kipf and Welling <https://arxiv.org/abs/1609.02907>`_. Here\n","is the simplest definition of a GCN framework. We recommend that you \n","read the original paper for more details.\n","\n","- At layer $l$, each node $v_i^l$ carries a feature vector $h_i^l$.\n","- Each layer of the GCN tries to aggregate the features from $u_i^{l}$ where\n","  $u_i$'s are neighborhood nodes to $v$ into the next layer representation at\n","  $v_i^{l+1}$. This is followed by an affine transformation with some\n","  non-linearity.\n","\n"," **In this project we use weighted graph convolution and weights are \n","learned during the trainning process.**\n","\n","The above definition of GCN fits into a **message-passing** paradigm: Each\n","node will update its own feature with information sent from neighboring\n","nodes. A graphical demonstration is displayed below.\n","\n","\n","In DGL, we provide implementations of popular Graph Neural Network layers under\n","the `dgl.<backend>.nn` subpackage. The :class:`~dgl.nn.pytorch.GraphConv` module\n","implements one Graph Convolutional layer."]},{"cell_type":"markdown","metadata":{"id":"eBIgnqqtoCGB","colab_type":"text"},"source":["**In our case, we want to perform edge classification based on feature inputs from nodes.**\n","* Thus we need to redesign the GNN to map the features from node to deges\n","* Define a deeper GCN model that contains two GCN layers:\n","\n","We first try convolutions on the nodes first then map to edges then try mapping to edges first then do convolutions on edges"]},{"cell_type":"code","metadata":{"id":"J2fkZxWsoCGP","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599495170106,"user_tz":300,"elapsed":361,"user":{"displayName":"Jeehyun Park","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GizNw5DW2QVCa5vUMiz0MiTNIYXIkGzaDCp0Es2=s64","userId":"14458427846939093236"}}},"source":["from torch.autograd import Variable\n","from dgl.nn.pytorch.conv import GraphConv\n","\n","\n","\n","# One layer Graph convolution from nodes to edges\n","class Graph_convolution_v2e(nn.Module):\n","    def __init__(self,in_features,out_features,W,bias=True):\n","        super(Graph_convolution_v2e,self).__init__()\n","#         self.Weight=nn.Parameter(torch.Tensor(W))\n","#         self.Weight=nn.Parameter(torch.from_numpy(W))\n","        self.register_buffer('w',torch.from_numpy(W).float())\n","#         for temp in self.Weight:\n","#             temp.requires_grad=False\n","#         self.Weight = self.Weight.detach()\n","        self.scale=nn.Parameter(torch.Tensor(out_features,1))\n","        self.bias=nn.Parameter(torch.Tensor(out_features,1))\n","    \n","    def forward(self,input):\n","        # print(input.shape)\n","        # print(self.scale.shape)  \n","        h = torch.matmul(Variable(self.w),input) \n","        return torch.mul(h,self.scale) + self.bias   \n","\n","# Need to change that to our case: 2 networks, one node clustering (or double it as in the reference), \n","# one node to edge.\n","\n","# Set dummy bus signals as zero\n","I_bus = np.identity(n_bus)\n","idx = [2,3,4,5,7,8,9,10,11,16,18,19,23]\n","for i in range(len(idx)):\n","  I_bus[idx,idx] = 0\n","I_bus = torch.from_numpy(I_bus).float()\n","\n","# GNN using DGL v2v graph convolution and our own v2e graph convolution\n","class GCN(nn.Module):\n","    def __init__(self, in_feats, hidden_size, num_nodes):\n","        super(GCN, self).__init__()\n","        self.conv1 = GraphConv(in_feats, hidden_size[0],norm='both', weight=True, bias=True, activation=None) \n","        #norm='both', weight=True, bias=True, activation=None\n","        self.conv2 = GraphConv(hidden_size[0], hidden_size[1],norm='both', weight=True, bias=True, activation=None)\n","        self.conv3 = GraphConv(hidden_size[1], hidden_size[2],norm='both', weight=True, bias=True, activation=None)\n","        self.conv4 = GraphConv(hidden_size[2], hidden_size[3],norm='both', weight=True, bias=True, activation=None)\n","        self.conv5 = GraphConv(hidden_size[3], hidden_size[4],norm='both', weight=True, bias=True, activation=None)\n","        self.conv6 = GraphConv(hidden_size[4], hidden_size[5],norm='both', weight=True, bias=True, activation=None)\n","        # self.conv7 = GraphConv(hidden_size[5], hidden_size[6],norm='both', weight=True, bias=True, activation=None)\n","        # self.conv8 = GraphConv(hidden_size[6], hidden_size[7],norm='both', weight=True, bias=True, activation=None)\n","        # self.conv9 = GraphConv(hidden_size[7], hidden_size[8],norm='both', weight=True, bias=True, activation=None)\n","        # self.conv3 = Graph_convolution_v2e(hidden_size[1], num_edges, W)\n","#         self.W = W\n","\n","        self.lin_output = nn.Linear(num_nodes,num_nodes)\n","\n","    def forward(self, g, I, inputs): # g is the graph stored in DGL\n","        # print(g) \n","        # nx_G = g.to_networkx().to_undirected()\n","        # pos = nx.kamada_kawai_layout(nx_G)\n","        # nx.draw(nx_G, pos, with_labels=True, node_color=[[.7, .7, .7]])      \n","        # print('inputs data type:', inputs.data.type())\n","        # print(inputs.shape)\n","        h = self.conv1(g, inputs)\n","        # h = torch.sigmoid(h)\n","        # print(h.shape)\n","        h = self.conv2(g, h)\n","        # print(h.shape)\n","        h = torch.sigmoid(h)\n","        # print(h.shape)\n","        h = self.conv3(g, h)\n","        # h = torch.sigmoid(h)\n","        h = self.conv4(g, h)\n","        h = torch.sigmoid(h) \n","        h = self.conv5(g, h)\n","        # h = torch.sigmoid(h)\n","        h = self.conv6(g, h)\n","        h = torch.sigmoid(h)\n","        # h = self.conv7(g, h)\n","        # # h = torch.sigmoid(h)\n","        # h = self.conv8(g, h)\n","        # h = torch.sigmoid(h)\n","        # h = self.conv9(g, h)\n","        # h = torch.sigmoid(h)\n","        # h = self.conv3(h)\n","        # h = torch.matmul(I,h)\n","        h = self.lin_output(h.transpose(0,1)).transpose(0,1)\n","        # h = torch.matmul(I,h)\n","        return h\n","\n"," \n","# net = GCN_self(n_bus, [10,10], n_line, W, W1)\n","# net = GCN(n_bus, [10,10,10,10,10,10,10,1], n_line, W1)\n","w_params = [40,80,80,80,80,1]\n","net = GCN(1, w_params, n_bus)"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"y_cb_xqOj9bq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":170},"executionInfo":{"status":"ok","timestamp":1599495178592,"user_tz":300,"elapsed":378,"user":{"displayName":"Jeehyun Park","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GizNw5DW2QVCa5vUMiz0MiTNIYXIkGzaDCp0Es2=s64","userId":"14458427846939093236"}},"outputId":"1e49d6fd-5dc9-4533-cabe-cbd9a2faf5b4"},"source":["net"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GCN(\n","  (conv1): GraphConv(in=1, out=40, normalization=both, activation=None)\n","  (conv2): GraphConv(in=40, out=80, normalization=both, activation=None)\n","  (conv3): GraphConv(in=80, out=80, normalization=both, activation=None)\n","  (conv4): GraphConv(in=80, out=80, normalization=both, activation=None)\n","  (conv5): GraphConv(in=80, out=80, normalization=both, activation=None)\n","  (conv6): GraphConv(in=80, out=1, normalization=both, activation=None)\n","  (lin_output): Linear(in_features=24, out_features=24, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"mpNqT4YmoCGg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1599495181187,"user_tz":300,"elapsed":306,"user":{"displayName":"Jeehyun Park","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GizNw5DW2QVCa5vUMiz0MiTNIYXIkGzaDCp0Es2=s64","userId":"14458427846939093236"}},"outputId":"2992762c-a4cc-4f3e-c7d6-98662cc3927f"},"source":["# Set the ratio of training/test set\n","train_ratio = 0.8\n","n_train = int(np.floor(train_ratio * n_sample))\n","n_test = n_sample - n_train\n","print('Data set size: ',n_sample,', training set size: ',n_train,', test set size: ',n_test)\n","\n","# Generate training set and test set\n","from sklearn.model_selection import train_test_split\n","x_train2, x_test2, y_train2, y_test2 = train_test_split(load_data[0:n_bus,:].transpose(), y0[:,0:n_sample].transpose(), test_size=0.2, random_state=22)\n","x_train = x_train2.transpose()\n","y_train = y_train2.transpose()\n","x_test = x_test2.transpose()\n","y_test = y_test2.transpose()\n","# x_train = load_data[0:n_bus,0:n_train].copy()\n","# y_train = bound_idx[:,0:n_train].copy()\n","# x_test = load_data[0:n_bus,n_train:n_sample].copy()\n","# y_test = bound_idx[:,n_train:n_sample].copy()\n","\n","\n","tempx=np.array([np.transpose(x_train)[0]])\n","x_train_one=torch.from_numpy(tempx).float()\n","x_train_one=x_train_one.transpose(0,1)\n","print(x_train_one.shape)\n","\n","\n","tempy=np.array([np.transpose(y_train)[0]])\n","y_train_one=torch.from_numpy(tempy).float()\n","y_train_one=y_train_one.transpose(0,1)\n","print(y_train_one.shape)\n","# print('Training data size:',x_train.shape)\n","# print('Training label size:',y_train.shape)\n","\n","# get the training data for DGL\n","def get_xy(index):\n","    tempx=np.array([np.transpose(x_train)[index]])\n","    x_train_one=torch.from_numpy(tempx).float()\n","    x_train_one=x_train_one.transpose(0,1)\n","\n","    tempy=np.array([np.transpose(y_train)[index]])\n","    y_train_one=torch.from_numpy(tempy).float()\n","    y_train_one=y_train_one.transpose(0,1)\n","\n","    return x_train_one,y_train_one\n","\n","# generate torch y_train for concatenated obj\n","y_train1 = torch.from_numpy(y_train)#.float()\n","# expand into 3d tensor \n","y_train1.unsqueeze_(-1)\n","y_train1 = y_train1.expand(n_bus,n_train,1)\n","y_train1 = y_train1.transpose(2,1)\n","print(y_train1.shape)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Data set size:  6000 , training set size:  4800 , test set size:  1200\n","torch.Size([24, 1])\n","torch.Size([24, 1])\n","torch.Size([24, 1, 4800])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vYRy5jCOpg7m","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1599495183815,"user_tz":300,"elapsed":325,"user":{"displayName":"Jeehyun Park","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GizNw5DW2QVCa5vUMiz0MiTNIYXIkGzaDCp0Es2=s64","userId":"14458427846939093236"}},"outputId":"1188e783-4bb7-4266-d2ee-7cfbd56a37e9"},"source":["# GNN model size\n","print('Network info:',net)\n","for temp in net.parameters():\n","    print('Parameter',temp,'size:',temp.shape)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Network info: GCN(\n","  (conv1): GraphConv(in=1, out=40, normalization=both, activation=None)\n","  (conv2): GraphConv(in=40, out=80, normalization=both, activation=None)\n","  (conv3): GraphConv(in=80, out=80, normalization=both, activation=None)\n","  (conv4): GraphConv(in=80, out=80, normalization=both, activation=None)\n","  (conv5): GraphConv(in=80, out=80, normalization=both, activation=None)\n","  (conv6): GraphConv(in=80, out=1, normalization=both, activation=None)\n","  (lin_output): Linear(in_features=24, out_features=24, bias=True)\n",")\n","Parameter Parameter containing:\n","tensor([[ 0.1608, -0.0842,  0.1558,  0.3755, -0.0128, -0.2417,  0.2242, -0.3600,\n","         -0.3071,  0.1024,  0.3479,  0.1534,  0.1103, -0.1028, -0.1813, -0.2341,\n","         -0.2464,  0.1902,  0.1403, -0.3542, -0.0145,  0.0610,  0.1362, -0.2841,\n","         -0.0855,  0.0213,  0.1494,  0.0183,  0.3378,  0.1657,  0.0104,  0.1959,\n","          0.3132, -0.2874, -0.0232,  0.0658, -0.2842, -0.1680, -0.2470,  0.2466]],\n","       requires_grad=True) size: torch.Size([1, 40])\n","Parameter Parameter containing:\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       requires_grad=True) size: torch.Size([40])\n","Parameter Parameter containing:\n","tensor([[-0.0261,  0.0834,  0.0466,  ...,  0.0368, -0.0444,  0.0401],\n","        [-0.0448,  0.0499,  0.1865,  ...,  0.1685, -0.1271, -0.0016],\n","        [ 0.1162, -0.1198,  0.1189,  ...,  0.1697, -0.1346,  0.1075],\n","        ...,\n","        [ 0.2059,  0.0153, -0.2191,  ...,  0.0999,  0.0783,  0.1752],\n","        [-0.2232,  0.0970, -0.0353,  ...,  0.2166, -0.1474, -0.1922],\n","        [ 0.0085, -0.0483, -0.1431,  ...,  0.0363, -0.0424,  0.0961]],\n","       requires_grad=True) size: torch.Size([40, 80])\n","Parameter Parameter containing:\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True) size: torch.Size([80])\n","Parameter Parameter containing:\n","tensor([[-0.1127, -0.1790,  0.1396,  ...,  0.1631, -0.1425, -0.1838],\n","        [ 0.1855,  0.0502,  0.0587,  ..., -0.1373, -0.0368,  0.0529],\n","        [ 0.1384,  0.0150, -0.1148,  ...,  0.1901,  0.1098, -0.0879],\n","        ...,\n","        [-0.1449, -0.0727,  0.1293,  ..., -0.0687, -0.0902, -0.1573],\n","        [ 0.0872,  0.0812,  0.0975,  ..., -0.1329, -0.0920,  0.1647],\n","        [-0.0678, -0.1232,  0.0912,  ..., -0.1016, -0.1432, -0.1860]],\n","       requires_grad=True) size: torch.Size([80, 80])\n","Parameter Parameter containing:\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True) size: torch.Size([80])\n","Parameter Parameter containing:\n","tensor([[ 0.1875,  0.1067, -0.0779,  ...,  0.1442,  0.0235, -0.0172],\n","        [-0.0196,  0.1255, -0.0050,  ..., -0.1688,  0.1379, -0.1388],\n","        [-0.1135,  0.1533, -0.1760,  ..., -0.1320,  0.1487, -0.0491],\n","        ...,\n","        [-0.0556,  0.1715, -0.1695,  ..., -0.0309, -0.0964,  0.0125],\n","        [ 0.0364, -0.0926,  0.0223,  ..., -0.1226,  0.1562, -0.1613],\n","        [-0.0998,  0.1252, -0.1886,  ...,  0.0759, -0.0897, -0.1002]],\n","       requires_grad=True) size: torch.Size([80, 80])\n","Parameter Parameter containing:\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True) size: torch.Size([80])\n","Parameter Parameter containing:\n","tensor([[ 0.0909, -0.1421, -0.0448,  ..., -0.1933, -0.1588,  0.0502],\n","        [ 0.0103, -0.1316,  0.1119,  ...,  0.0158,  0.0118, -0.0646],\n","        [ 0.0958,  0.1102, -0.0339,  ...,  0.0025, -0.1533, -0.1191],\n","        ...,\n","        [-0.0654,  0.0621, -0.0108,  ..., -0.0581,  0.0841,  0.1919],\n","        [ 0.0004,  0.1650, -0.0461,  ..., -0.1926,  0.0430, -0.0846],\n","        [-0.0507,  0.0259, -0.0357,  ..., -0.1878, -0.1603,  0.0313]],\n","       requires_grad=True) size: torch.Size([80, 80])\n","Parameter Parameter containing:\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True) size: torch.Size([80])\n","Parameter Parameter containing:\n","tensor([[-0.1978],\n","        [ 0.0862],\n","        [ 0.2128],\n","        [ 0.1403],\n","        [-0.2505],\n","        [ 0.2365],\n","        [ 0.0036],\n","        [-0.1294],\n","        [ 0.0292],\n","        [-0.2406],\n","        [-0.0453],\n","        [-0.1756],\n","        [-0.0650],\n","        [-0.1669],\n","        [-0.1001],\n","        [ 0.1328],\n","        [-0.0903],\n","        [-0.0470],\n","        [-0.2611],\n","        [ 0.0669],\n","        [-0.2664],\n","        [-0.2155],\n","        [-0.2095],\n","        [ 0.0207],\n","        [-0.2089],\n","        [ 0.2321],\n","        [-0.2335],\n","        [-0.0015],\n","        [ 0.0239],\n","        [ 0.2279],\n","        [ 0.2541],\n","        [ 0.1894],\n","        [-0.2677],\n","        [-0.2354],\n","        [ 0.0616],\n","        [ 0.1547],\n","        [ 0.1125],\n","        [ 0.0964],\n","        [ 0.0171],\n","        [ 0.0178],\n","        [ 0.2500],\n","        [-0.0267],\n","        [-0.2451],\n","        [ 0.0280],\n","        [ 0.2098],\n","        [ 0.0232],\n","        [ 0.0912],\n","        [ 0.1225],\n","        [ 0.0355],\n","        [-0.0471],\n","        [ 0.0648],\n","        [ 0.1746],\n","        [ 0.2536],\n","        [ 0.0964],\n","        [-0.2384],\n","        [ 0.0160],\n","        [ 0.1426],\n","        [-0.0585],\n","        [ 0.0112],\n","        [-0.0652],\n","        [ 0.0708],\n","        [-0.1086],\n","        [ 0.0131],\n","        [ 0.2333],\n","        [ 0.0993],\n","        [ 0.1253],\n","        [-0.1162],\n","        [ 0.2539],\n","        [ 0.1742],\n","        [-0.2083],\n","        [-0.0676],\n","        [ 0.0598],\n","        [ 0.0424],\n","        [-0.2633],\n","        [ 0.2633],\n","        [-0.2006],\n","        [ 0.2523],\n","        [-0.2105],\n","        [ 0.0492],\n","        [-0.0115]], requires_grad=True) size: torch.Size([80, 1])\n","Parameter Parameter containing:\n","tensor([0.], requires_grad=True) size: torch.Size([1])\n","Parameter Parameter containing:\n","tensor([[ 4.2980e-02,  1.9916e-01, -9.9069e-03,  1.6333e-01,  1.9688e-01,\n","         -1.9649e-01,  1.7582e-01, -1.4888e-01,  1.4874e-01,  1.7322e-01,\n","         -8.9829e-02,  1.6213e-01,  1.3685e-01, -1.9927e-01,  1.6816e-01,\n","          1.9409e-01,  9.1421e-02,  3.7484e-02, -1.5738e-01,  1.3073e-01,\n","          8.2012e-02,  7.7408e-02,  1.0030e-01, -1.0281e-01],\n","        [-4.8022e-02,  1.9298e-02,  1.6058e-01, -1.3549e-01, -1.7776e-01,\n","          3.5090e-03, -4.9238e-03,  4.4870e-02, -9.1232e-02, -1.0374e-01,\n","         -2.2490e-03,  1.7423e-01,  4.3032e-02, -1.2468e-01,  9.2627e-02,\n","         -9.9691e-02, -7.1232e-02, -1.5051e-01,  1.5771e-01,  1.9467e-01,\n","          1.8512e-02, -1.0132e-01, -8.8122e-03,  1.0358e-01],\n","        [-1.0733e-01, -5.3197e-02,  2.7535e-02,  8.7617e-02,  8.2390e-03,\n","         -7.8342e-02,  1.6322e-01, -1.1407e-01, -1.3078e-01,  1.9225e-01,\n","          1.6503e-01, -1.1371e-01,  4.8384e-02,  1.5660e-01,  1.1932e-02,\n","         -9.4856e-02,  6.6393e-02,  2.6714e-02,  1.5940e-01, -6.1327e-02,\n","          1.3192e-01,  1.0354e-01,  1.1914e-01,  1.6903e-01],\n","        [ 3.7664e-02,  1.9534e-01,  5.6567e-02,  4.0472e-02, -1.6720e-01,\n","         -1.4363e-01,  1.2100e-01,  1.7517e-01,  1.8578e-01, -1.1596e-01,\n","         -1.7718e-02,  1.3995e-01,  1.9838e-01,  1.2419e-01, -3.4927e-02,\n","          6.3441e-02, -1.5714e-01,  1.7732e-01, -4.9843e-02,  1.1366e-01,\n","         -1.9250e-01, -1.7669e-01, -2.7726e-02, -7.2908e-02],\n","        [ 2.0381e-01,  2.4330e-02,  1.6661e-01, -9.2063e-02,  1.3937e-01,\n","          1.8357e-01,  6.4699e-02,  1.9528e-01, -1.9361e-01,  5.8079e-02,\n","          9.3563e-02,  4.3714e-02, -8.6681e-02, -1.9117e-01,  8.5052e-02,\n","         -8.2158e-02, -1.6058e-01,  5.3906e-02,  3.1463e-02,  1.9605e-02,\n","          1.2225e-01,  1.1838e-01, -1.9944e-01,  7.8844e-03],\n","        [ 1.4163e-01, -1.1755e-01,  9.8479e-02,  2.7798e-02, -1.8683e-01,\n","         -1.8943e-02,  5.9749e-02, -1.7132e-01, -1.8673e-01,  3.5483e-02,\n","         -1.5536e-01,  1.3977e-01, -6.1064e-02, -1.3546e-01, -1.5593e-01,\n","         -1.6347e-01,  1.5700e-01, -3.4900e-02, -5.4212e-03,  1.4509e-01,\n","          1.6982e-01, -1.7809e-01,  1.2931e-01,  3.6429e-02],\n","        [ 1.6959e-01, -1.1427e-01, -1.0486e-01,  1.1488e-01, -1.7897e-01,\n","          8.4327e-02,  1.2079e-01,  1.1267e-01,  1.4540e-01, -1.4612e-01,\n","         -4.8902e-02,  5.8543e-02,  1.1736e-01, -1.9450e-01,  1.2108e-01,\n","          1.5958e-01,  1.3229e-01, -3.6064e-02,  1.9143e-01, -1.9525e-01,\n","          3.7118e-02,  1.1427e-01,  1.8276e-01, -5.7172e-02],\n","        [-2.3040e-02, -3.9395e-02, -8.9622e-02, -1.9662e-01,  4.4458e-02,\n","          6.3528e-03,  1.7634e-01, -3.0252e-02, -1.3703e-01, -3.1437e-02,\n","          2.1591e-04,  2.7134e-02,  6.5141e-04,  1.6043e-01, -1.4566e-01,\n","         -6.4388e-02,  3.9034e-02, -1.9148e-01,  1.5896e-01, -7.2072e-02,\n","         -8.9619e-02, -1.1525e-02,  1.0072e-01, -2.0204e-01],\n","        [-7.3987e-02, -1.9412e-01,  2.0779e-02, -1.7891e-02,  1.1399e-01,\n","         -1.6716e-01,  6.3784e-02,  6.1871e-02, -1.4848e-01,  4.4050e-02,\n","          1.7732e-01,  6.6347e-02,  9.4679e-02,  3.6448e-02,  2.9996e-02,\n","         -1.2465e-01,  1.0997e-01, -3.0300e-02, -1.5362e-01, -1.0934e-01,\n","         -7.4591e-02,  1.7564e-01, -1.7945e-01,  1.4883e-01],\n","        [ 1.3315e-03,  2.9308e-02,  8.6361e-02, -1.4623e-01, -8.3077e-03,\n","          1.2069e-01,  2.8661e-02, -3.5349e-02, -1.8503e-01,  1.0124e-03,\n","         -1.9762e-01,  1.3400e-01, -7.8913e-02, -9.7813e-02, -1.4235e-01,\n","         -2.9316e-02, -7.2811e-02, -8.1500e-02,  1.3919e-01, -5.2891e-02,\n","         -1.6402e-01,  1.6414e-01, -1.0225e-01,  1.9884e-01],\n","        [ 5.5485e-02,  1.9535e-01,  1.5117e-01, -1.3859e-01,  6.0721e-03,\n","         -1.0958e-01,  1.7260e-02,  7.7559e-02, -1.9725e-01, -1.1814e-01,\n","         -1.0507e-01,  1.1577e-01,  1.8387e-01, -1.0441e-01, -1.0319e-01,\n","          7.1599e-02, -1.6294e-02, -9.1796e-02,  2.5504e-02,  1.0057e-01,\n","          8.3681e-02, -2.9234e-02, -1.8648e-01, -9.8482e-02],\n","        [-8.7691e-02,  3.6554e-03,  1.8510e-01,  2.5990e-02, -1.7456e-01,\n","          5.7933e-02,  1.1632e-01,  1.9221e-01,  1.2880e-01,  1.5129e-01,\n","          3.7681e-02, -1.3856e-01,  1.4118e-01,  1.6575e-01, -1.3385e-02,\n","         -5.8088e-02,  7.9196e-02,  3.4260e-02, -8.7078e-02, -1.6932e-01,\n","          1.9490e-01, -9.2248e-02,  1.3541e-01, -2.1906e-02],\n","        [-2.0028e-01, -6.2543e-02,  1.7632e-01,  1.6638e-01,  1.6643e-02,\n","          1.3355e-02,  1.8859e-01,  1.5463e-01, -1.2582e-01, -5.3287e-02,\n","          1.1063e-01,  7.2284e-03, -5.4971e-03,  1.9304e-01,  4.3367e-02,\n","         -1.2126e-01,  1.7612e-02,  1.2227e-01,  3.4732e-02, -9.8801e-02,\n","          3.2508e-02, -7.1521e-03,  1.9018e-02, -7.3718e-03],\n","        [-3.8357e-03, -6.9109e-02, -4.7956e-02, -9.0334e-02,  9.0156e-02,\n","          1.5838e-01, -1.7988e-01, -1.0618e-01, -1.1635e-01, -1.6012e-01,\n","         -8.8284e-02, -1.2119e-02, -1.2794e-01,  1.1380e-01,  1.7499e-01,\n","          1.6023e-02,  7.5296e-02, -1.2336e-01, -1.7977e-01, -1.7237e-01,\n","          1.4881e-01, -7.5290e-02,  1.4892e-01, -1.1052e-01],\n","        [ 1.5717e-02, -1.3482e-01,  3.4062e-04,  1.7124e-01, -3.5017e-02,\n","          1.3830e-01, -1.3985e-01, -1.4641e-01, -1.3229e-01, -4.8101e-03,\n","         -6.0833e-02, -1.9985e-01,  4.0971e-02, -5.4784e-02, -1.0172e-02,\n","          1.0956e-02, -8.7266e-02, -8.5483e-02, -1.0580e-01, -1.4576e-01,\n","         -1.5870e-01, -1.5598e-01,  2.8766e-02,  1.1185e-01],\n","        [ 9.9130e-02, -1.7420e-01,  5.2973e-02,  4.7210e-02, -1.9597e-01,\n","         -7.8622e-02, -9.1526e-02,  1.7497e-01, -5.4732e-02, -1.2382e-01,\n","         -1.3755e-01, -1.5916e-01,  1.4307e-01,  7.8506e-02, -8.3709e-02,\n","          2.0164e-01,  5.7870e-02, -1.9097e-01,  1.8862e-01,  8.6822e-02,\n","          1.2319e-01, -1.1929e-01, -1.9230e-01, -1.3507e-01],\n","        [-1.5722e-01, -1.8158e-01,  1.4513e-01,  9.3921e-02, -1.9810e-02,\n","          4.6390e-02,  2.5936e-02,  1.6319e-01,  5.5520e-02, -4.4969e-02,\n","         -2.4272e-02,  1.0227e-02, -1.2646e-01,  1.4480e-02, -8.4673e-02,\n","         -5.6435e-02, -4.8690e-02, -1.5252e-01, -5.8579e-02,  5.0298e-02,\n","          8.2895e-02, -1.9989e-01, -1.2837e-01, -1.1071e-01],\n","        [-1.6676e-01,  3.2160e-02, -1.0914e-01,  1.2833e-01,  1.8328e-01,\n","         -6.6794e-02,  7.4219e-02,  1.0422e-01, -3.6393e-02,  1.6330e-01,\n","          1.2317e-01,  1.6912e-01, -8.0966e-02, -1.4991e-01, -1.5577e-01,\n","          1.0089e-01, -1.8345e-01,  1.5460e-01, -1.2236e-01, -8.4446e-02,\n","         -1.5430e-01,  5.4306e-02,  1.9214e-01, -1.8424e-01],\n","        [ 1.0461e-03, -1.0117e-01, -1.9339e-01, -1.4407e-01, -1.2798e-01,\n","         -1.8704e-01,  6.6352e-02,  1.6282e-01, -1.2586e-02, -1.1127e-01,\n","          1.2688e-01,  1.6509e-01, -1.0847e-02, -1.0152e-01,  4.9078e-02,\n","          1.0933e-03, -3.3214e-02,  2.9717e-03,  8.3113e-02,  2.3884e-02,\n","          8.5494e-02, -6.8678e-02,  2.6704e-02,  1.3095e-01],\n","        [-1.1728e-01, -6.0565e-02, -7.0372e-02,  2.4479e-02, -2.1014e-03,\n","         -4.0545e-02,  4.2129e-02,  1.0153e-01, -1.5878e-01,  1.3702e-01,\n","         -1.9269e-01,  1.6660e-01, -1.8617e-01,  1.7125e-01,  1.8037e-01,\n","          1.1445e-01, -1.3873e-01,  1.7890e-01,  9.6092e-02,  1.1305e-01,\n","          9.9383e-03,  9.8333e-02,  7.4740e-02,  9.9087e-02],\n","        [-4.1790e-02,  1.5871e-01, -8.0774e-02, -1.5119e-01, -7.4678e-02,\n","         -5.5384e-02,  3.9570e-02,  1.8504e-01,  1.7492e-01, -1.0647e-01,\n","          1.8823e-01,  1.4226e-01,  6.9060e-02,  7.7435e-03,  1.5851e-01,\n","          2.0411e-01,  6.7849e-02,  1.7219e-01,  3.4584e-02, -1.1283e-03,\n","         -1.2666e-01,  1.9257e-01,  1.1080e-01,  3.8692e-03],\n","        [-1.7067e-01,  1.0618e-01,  3.6270e-03, -2.0872e-02,  1.1915e-01,\n","          7.4677e-02,  1.2447e-01, -3.6335e-02, -9.8636e-02,  1.6427e-01,\n","          6.5438e-02, -1.3500e-01, -6.7190e-02, -3.8714e-02, -1.0848e-01,\n","          5.6064e-02,  5.0492e-02, -2.9832e-02, -2.0367e-01,  1.6153e-01,\n","         -2.9326e-02,  1.4984e-01, -4.3693e-03,  1.7529e-01],\n","        [ 9.7579e-02, -8.3999e-05,  1.8411e-01,  1.5436e-01,  1.3659e-01,\n","          3.6688e-03,  1.2743e-01, -1.7638e-01,  8.5985e-02, -5.5217e-02,\n","         -1.0006e-01, -1.1619e-01, -7.5391e-02, -4.7976e-03, -4.0623e-02,\n","         -1.2805e-01,  1.1304e-01, -1.7651e-02,  9.6607e-02,  1.2091e-01,\n","          1.1729e-01, -6.6541e-02,  1.3977e-01, -1.1812e-01],\n","        [ 2.6152e-02,  1.2490e-01, -5.6753e-02, -7.0654e-02,  1.6664e-01,\n","          1.5757e-02,  1.0650e-01, -9.7735e-02, -1.6846e-01, -1.2088e-01,\n","         -9.7855e-02, -1.6175e-01, -1.4364e-01, -5.8114e-02,  1.1011e-02,\n","         -1.5912e-02, -4.8054e-02, -1.7375e-01,  1.2400e-01,  1.7817e-01,\n","         -1.6609e-01, -1.9309e-01, -1.2785e-01, -2.0341e-01]],\n","       requires_grad=True) size: torch.Size([24, 24])\n","Parameter Parameter containing:\n","tensor([ 0.0572,  0.1092,  0.0780, -0.0194, -0.0197, -0.1849,  0.1950,  0.0849,\n","         0.1615,  0.0079,  0.1403,  0.0918,  0.1441, -0.0606,  0.1655,  0.0411,\n","        -0.0675, -0.0727,  0.0678,  0.1984, -0.1734,  0.0368, -0.1271, -0.1643],\n","       requires_grad=True) size: torch.Size([24])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7uHUoHNZoCGq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1599499509571,"user_tz":300,"elapsed":4321761,"user":{"displayName":"Jeehyun Park","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GizNw5DW2QVCa5vUMiz0MiTNIYXIkGzaDCp0Es2=s64","userId":"14458427846939093236"}},"outputId":"4990d41b-96f9-4a42-c6c0-1e0888aa55d5"},"source":["# import itertools\n","# optimizer = torch.optim.Adam(itertools.chain(net.parameters(), embed.parameters()), lr=0.01)\n","device = torch.device('cpu')\n","\n","accumulation_steps = 100\n","optimizer = torch.optim.Adam(net.parameters())\n","loss_optm = []\n","ep_range = 100\n","for epoch in range(ep_range):\n","    # print('Epoch:', epoch,'LR:', scheduler.get_lr())\n","    for sample in range(n_train):\n","      x_train_one,y_train_one=get_xy(sample)\n","      net.train()\n","      logits = net(G, I_bus, x_train_one)\n","      # loss = F.binary_cross_entropy(torch.sigmoid(logits), y_train_one) #/ accumulation_steps\n","      # loss = F.mse_loss(torch.sigmoid(logits), y_train_one) #/ accumulation_steps\n","      loss = F.mse_loss(logits, y_train_one) #/ accumulation_steps\n","      loss.backward()\n","\n","      if ((sample+1)%accumulation_steps)==0:\n","        optimizer.step() # update parameters of net\n","        optimizer.zero_grad() # clear the psat gradient\n","\n","  # test for simple adaptive scheme\n","    # if epoch > 10:\n","    #   optimizer = torch.optim.Adam(net.parameters(), lr=0.15)\n","    # if epoch > 10:\n","    #   optimizer = torch.optim.Adam(net.parameters(), lr=0.04)\n","    # if epoch > 20:\n","    #   optimizer = torch.optim.Adam(net.parameters(), lr=0.03)\n","    # if epoch > 30:\n","    #   optimizer = torch.optim.Adam(net.parameters(), lr=0.02)\n","    # if epoch > 40:\n","    #   optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n","\n","    # # Decay Learning Rate\n","    # if epoch > 1:\n","    #   scheduler.step()\n","    \n","    print('Epoch %d | Loss: %.4f' % (epoch, loss.item()))\n","    loss_optm.append(loss.item())"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Epoch 0 | Loss: 0.0405\n","Epoch 1 | Loss: 0.0082\n","Epoch 2 | Loss: 0.0091\n","Epoch 3 | Loss: 0.0075\n","Epoch 4 | Loss: 0.0087\n","Epoch 5 | Loss: 0.0082\n","Epoch 6 | Loss: 0.0072\n","Epoch 7 | Loss: 0.0079\n","Epoch 8 | Loss: 0.0090\n","Epoch 9 | Loss: 0.0086\n","Epoch 10 | Loss: 0.0075\n","Epoch 11 | Loss: 0.0081\n","Epoch 12 | Loss: 0.0087\n","Epoch 13 | Loss: 0.0097\n","Epoch 14 | Loss: 0.0073\n","Epoch 15 | Loss: 0.0083\n","Epoch 16 | Loss: 0.0097\n","Epoch 17 | Loss: 0.0069\n","Epoch 18 | Loss: 0.0077\n","Epoch 19 | Loss: 0.0078\n","Epoch 20 | Loss: 0.0072\n","Epoch 21 | Loss: 0.0072\n","Epoch 22 | Loss: 0.0067\n","Epoch 23 | Loss: 0.0063\n","Epoch 24 | Loss: 0.0062\n","Epoch 25 | Loss: 0.0062\n","Epoch 26 | Loss: 0.0057\n","Epoch 27 | Loss: 0.0058\n","Epoch 28 | Loss: 0.0056\n","Epoch 29 | Loss: 0.0070\n","Epoch 30 | Loss: 0.0054\n","Epoch 31 | Loss: 0.0054\n","Epoch 32 | Loss: 0.0052\n","Epoch 33 | Loss: 0.0052\n","Epoch 34 | Loss: 0.0052\n","Epoch 35 | Loss: 0.0050\n","Epoch 36 | Loss: 0.0049\n","Epoch 37 | Loss: 0.0049\n","Epoch 38 | Loss: 0.0050\n","Epoch 39 | Loss: 0.0046\n","Epoch 40 | Loss: 0.0046\n","Epoch 41 | Loss: 0.0044\n","Epoch 42 | Loss: 0.0044\n","Epoch 43 | Loss: 0.0043\n","Epoch 44 | Loss: 0.0042\n","Epoch 45 | Loss: 0.0041\n","Epoch 46 | Loss: 0.0040\n","Epoch 47 | Loss: 0.0038\n","Epoch 48 | Loss: 0.0038\n","Epoch 49 | Loss: 0.0037\n","Epoch 50 | Loss: 0.0038\n","Epoch 51 | Loss: 0.0035\n","Epoch 52 | Loss: 0.0035\n","Epoch 53 | Loss: 0.0033\n","Epoch 54 | Loss: 0.0033\n","Epoch 55 | Loss: 0.0032\n","Epoch 56 | Loss: 0.0030\n","Epoch 57 | Loss: 0.0030\n","Epoch 58 | Loss: 0.0029\n","Epoch 59 | Loss: 0.0029\n","Epoch 60 | Loss: 0.0028\n","Epoch 61 | Loss: 0.0028\n","Epoch 62 | Loss: 0.0027\n","Epoch 63 | Loss: 0.0026\n","Epoch 64 | Loss: 0.0025\n","Epoch 65 | Loss: 0.0025\n","Epoch 66 | Loss: 0.0026\n","Epoch 67 | Loss: 0.0024\n","Epoch 68 | Loss: 0.0026\n","Epoch 69 | Loss: 0.0022\n","Epoch 70 | Loss: 0.0022\n","Epoch 71 | Loss: 0.0023\n","Epoch 72 | Loss: 0.0023\n","Epoch 73 | Loss: 0.0021\n","Epoch 74 | Loss: 0.0022\n","Epoch 75 | Loss: 0.0020\n","Epoch 76 | Loss: 0.0021\n","Epoch 77 | Loss: 0.0019\n","Epoch 78 | Loss: 0.0020\n","Epoch 79 | Loss: 0.0019\n","Epoch 80 | Loss: 0.0020\n","Epoch 81 | Loss: 0.0018\n","Epoch 82 | Loss: 0.0019\n","Epoch 83 | Loss: 0.0018\n","Epoch 84 | Loss: 0.0019\n","Epoch 85 | Loss: 0.0018\n","Epoch 86 | Loss: 0.0018\n","Epoch 87 | Loss: 0.0017\n","Epoch 88 | Loss: 0.0018\n","Epoch 89 | Loss: 0.0017\n","Epoch 90 | Loss: 0.0018\n","Epoch 91 | Loss: 0.0017\n","Epoch 92 | Loss: 0.0017\n","Epoch 93 | Loss: 0.0017\n","Epoch 94 | Loss: 0.0017\n","Epoch 95 | Loss: 0.0017\n","Epoch 96 | Loss: 0.0017\n","Epoch 97 | Loss: 0.0016\n","Epoch 98 | Loss: 0.0018\n","Epoch 99 | Loss: 0.0016\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LSw7m9Tos9MH","colab_type":"code","colab":{}},"source":["ep_range1 = 20\n","for epoch in range(ep_range1):\n","    # print('Epoch:', epoch,'LR:', scheduler.get_lr())\n","    for sample in range(n_train):\n","      x_train_one,y_train_one=get_xy(sample)\n","      net.train()\n","      logits = net(G,I_bus, x_train_one)\n","      # loss = F.binary_cross_entropy(torch.sigmoid(logits), y_train_one) #/ accumulation_steps\n","      # loss = F.mse_loss(torch.sigmoid(logits), y_train_one) #/ accumulation_steps\n","      loss = F.mse_loss(logits, y_train_one) #/ accumulation_steps\n","      loss.backward()\n","\n","      if ((sample+1)%accumulation_steps)==0:\n","        optimizer.step() # update parameters of net\n","        optimizer.zero_grad() # clear the psat gradient\n","\n","    \n","    print('Epoch %d | Loss: %.4f' % (epoch+ep_range, loss.item()))\n","    loss_optm.append(loss.item())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zZ0nLutzpyHX","colab_type":"text"},"source":["## Optimization methods:\n","**We need to incorporate batch size into the optimization step size for mini batches.** \n","- **Rprop:** A gradient descent algorithm that only uses the signs of gradients to compute updates. It stands for Resilient Propagation and works well in many situations because it adapts the step size dynamically for each weight independently.\n","- **Learning rate scheduling:** used with optimizers\n","- **lr_scheduler.StepLR:** Decays the learning rate of each parameter group by gamma every step_size epochs. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When last_epoch=-1, sets initial lr as lr. https://www.deeplearningwizard.com/deep_learning/boosting_models_pytorch/lr_scheduling/\n","- **lr_scheduler.CyclicLR:** Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). The policy cycles the learning rate between two boundaries with a constant frequency. Cyclical learning rate policy changes the learning rate after every batch. step should be called after a batch has been used for training."]},{"cell_type":"code","metadata":{"id":"pfTSSX29R7jP","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","plt.figure()\n","plot_idx = np.arange(np.size(loss_optm))\n","plt.plot(plot_idx[3:-1],loss_optm[3:-1],lw=2,label='loss level')\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.legend()\n","plt.show(block=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iF-gZIOko2Jd","colab_type":"text"},"source":["## Step 4. Validation\n","**Validate the trained model using the test set**"]},{"cell_type":"markdown","metadata":{"id":"2uCp3fEEFGCv","colab_type":"text"},"source":["When a model is trained, we can use the following method to evaluate the performance of the model on the test dataset:"]},{"cell_type":"code","metadata":{"id":"nrhVrVUb0d2t","colab_type":"code","colab":{}},"source":["# Generate one data pt. for testing the trained model\n","def get_xy_test(index):\n","    tempx=np.array([np.transpose(x_test)[index]])\n","    x_train_one=torch.from_numpy(tempx).float()\n","    x_train_one=x_train_one.transpose(0,1)\n","\n","    tempy=np.array([np.transpose(y_test)[index]])\n","    y_train_one=torch.from_numpy(tempy).float()\n","    y_train_one=y_train_one.transpose(0,1)\n","\n","    return x_train_one,y_train_one\n","\n","# generate torch y_train for concatenated obj\n","y_test1 = torch.from_numpy(y_test)#.float()\n","# expand into 3d tensor \n","y_test1.unsqueeze_(-1)\n","y_test1 = y_test1.expand(n_bus,n_test,1)\n","y_test1 = y_test1.transpose(2,1)\n","print(y_test1.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C5PBICs8E51B","colab_type":"code","colab":{}},"source":["# model evaluation function\n","def evaluate(model, g, features, labels, n, I):\n","    model.eval()\n","    with torch.no_grad():\n","      indices = torch.tensor(np.zeros((n_bus,1,n)))\n","      for sample in range(n):\n","        x_sample,y_sample=get_xy_test(sample)\n","        logits = model(g, I, x_sample)\n","        logp = logits\n","        # logp = torch.sigmoid(logits)\n","        # logp = F.log_softmax(logits, 1)\n","        # _, indices1 = torch.max(logp, dim=1)\n","        indices[:,:,sample] = logp.clone()\n","\n","      # correct = torch.sum(indices == labels)\n","      # return correct.item() * 1.0 / len(labels)\n","      return indices"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L5VvNfm_cFMD","colab_type":"code","colab":{}},"source":["print(x_test.shape)\n","print(y_test1.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YrE4bhDc64fK","colab_type":"code","colab":{}},"source":["# make predictions\n","indices = evaluate(net, G, x_test, y_test1,n_test, I_bus)\n","# test_acc = evaluate(net, G, x_test, y_test1,n_test)\n","# print(\"Epoch_last {:05d} | Loss {:.4f} | Test Acc {:.4f}\".format(epoch, loss.item(), test_acc))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"S7QRkoh8b1cm","colab":{}},"source":["y_test0 = y_test1.numpy().copy()\n","y_pred0 = indices.numpy().copy()\n","\n","L_infty_err = []\n","L1_err = []\n","for i in range(n_test):\n","  L_infty_err.append(np.max(np.abs(y_test0[:,:,i]-y_pred0[:,:,i])))\n","  L1_err.append(np.sum(np.abs(y_test0[:,:,i]-y_pred0[:,:,i])))\n","print('L_infty mean: ',np.mean(L_infty_err))\n","print('L_1 mean: ',np.mean(L1_err))\n"," \n","val, idx = min((val, idx) for (idx, val) in enumerate(L_infty_err))\n","print(val,idx)\n","# print('L_inf sample: \\n',y_test[:,:,idx])\n","# print(y_pred[:,:,idx])\n","val1, idx1 = min((val, idx) for (idx, val) in enumerate(L1_err))\n","print(val1,idx1)\n","# print('L1 sample: \\n',y_test[:,:,idx1])\n","# print(y_pred[:,:,idx1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CW9qX0nGgoS_","colab_type":"code","colab":{}},"source":["print(load_data[0:n_bus,:].shape)\n","print(y0[0:n_bus,0:n_sample].shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3pQ1v0b58NRH","colab_type":"code","colab":{}},"source":["# Save the predictions for other training/evaluation jobs\n","\n","import pickle\n","import pprint\n","from os import path\n","\n","from datetime import datetime\n","from packaging import version\n","\n","def save_dataset(test_case, dataset):\n","    file_name = test_case.split('.')[0]\n","    file_path = 'drive/My Drive/gnn/numerical_results/'\n","    file_dir = file_path + file_name + '.pickle'\n","    outfile = open(file_dir, 'wb')\n","    pickle.dump(dataset, outfile)\n","    outfile.close()\n","\n","def get_xy_eval(index):\n","    tempx=np.array([np.transpose(load_data[0:n_bus,:])[index]])\n","    x_train_one=torch.from_numpy(tempx).float()\n","    x_train_one=x_train_one.transpose(0,1)\n","\n","    return x_train_one\n","\n","def model_eval(model, g, features, n, I):\n","    model.eval()\n","    with torch.no_grad():\n","      indices = torch.tensor(np.zeros((n_bus,1,n)))\n","      for sample in range(n):\n","        x_sample=get_xy_eval(sample)\n","        logits = model(g, I, x_sample)\n","        logp = logits\n","        indices[:,:,sample] = logp.clone()\n","\n","      return indices\n","\n","indices = model_eval(net, G, load_data[0:n_bus,:], n_sample, I_bus)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NdH_Svt5js05","colab_type":"code","colab":{}},"source":["# generation prediction by GNN model\n","print(indices.shape)\n","gen_pred_norm = indices.numpy().copy()\n","\n","y0[j,i] = np.divide(gen_exact[j,i]-gen_low_limit[j],gen_up_limit[j]-gen_low_limit[j])\n","\n","gen_pred = np.zeros((n_bus,n_sample))\n","for i in range(n_sample):\n","  for j in range(n_bus):\n","    gen_pred[j,i] = indices[j,0,i] * (gen_up_limit[j]-gen_low_limit[j]) + gen_low_limit[j]\n","\n","dataset = {'pred': gen_pred}\n","# print(dataset)\n","filename = 'GNN_24bus_gen_prediction_copy_new'\n","save_dataset(filename, dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lZ5hVfqjde3z","colab_type":"code","colab":{}},"source":["for i in range(10):\n","  print(np.sum(x_train[:,i]),np.sum(gen_pred[:,i]))\n","# print(np.sum(gen_pred[:,1]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0h31WYc4LtTG","colab_type":"code","colab":{}},"source":["# Evaluate the prediction accuracy by relative error\n","acc_threshold1 = 0.05\n","acc_threshold = 0.1\n","\n","acc_count = 0\n","acc_count1 = 0\n","for i in range(24):\n","  if np.abs(y_test0[i,:,idx]) > 0:\n","    if np.abs(y_test0[i,:,idx]-y_pred0[i,:,idx])/np.abs(y_test0[i,:,idx]) < acc_threshold or np.abs(y_test0[i,:,idx]-y_pred0[i,:,idx]) < acc_threshold1:\n","        acc_count = acc_count+1\n","    if np.abs(y_test0[i,:,idx1]-y_pred0[i,:,idx1])/np.abs(y_test0[i,:,idx1]) < acc_threshold or np.abs(y_test0[i,:,idx1]-y_pred0[i,:,idx1]) < acc_threshold1:\n","        acc_count1 = acc_count1+1\n","  else:\n","    if np.abs(y_test0[i,:,idx]-y_pred0[i,:,idx]) < acc_threshold1:\n","        acc_count = acc_count+1\n","    if np.abs(y_test0[i,:,idx1]-y_pred0[i,:,idx1]) < acc_threshold1:\n","        acc_count1 = acc_count1+1\n","print('most accurate L_inf: ',acc_count,'  L_1: ',acc_count1)\n","\n","\n","acc_test = np.zeros(y_test0.shape[0])\n","for j in range(y_test0.shape[0]):\n","  acc_count = 0\n","  for i in range(24):\n","    if np.abs(y_test0[i,:,j]) > 0:\n","      if np.abs(y_test0[i,:,j]-y_pred0[i,:,j])/np.abs(y_test0[i,:,j]) < acc_threshold or np.abs(y_test0[i,:,j]-y_pred0[i,:,j]) < acc_threshold1:\n","          acc_count = acc_count+1\n","    else:\n","      if np.abs(y_test0[i,:,j]-y_pred0[i,:,j]) < acc_threshold1:\n","          acc_count = acc_count+1\n","  acc_test[j] = acc_count\n","print('Mean accuracy: ',np.mean(acc_test)) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SzHeOYpbIZ41","colab_type":"code","colab":{}},"source":["\n","# for i in range(3):\n","  # print('Prediction:',indices[:,:,i].transpose(0,1))\n","  # print('Ture label:',y_test1[:,:,i].transpose(0,1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eu4vxBiiG7IN","colab_type":"text"},"source":["## Data visualization"]},{"cell_type":"code","metadata":{"id":"kZIQUKv3G42U","colab_type":"code","colab":{}},"source":["import math\n","import seaborn as sns\n","def result_reshape(data):\n","    result_dim = math.ceil(math.sqrt(len(data)))\n","    reshaped_data = np.zeros((result_dim, result_dim))\n","\n","    for i in range(result_dim):\n","        for j in range(result_dim):\n","            try:\n","                reshaped_data[i][j] = data[result_dim * i + j]\n","            except IndexError:\n","                reshaped_data[i][j] = -1\n","    return reshaped_data\n","\n","\n","def test_vs_pred(y_test, y_pred,  data_idx, test_case):\n","    y_test0 = y_test[:,:,data_idx]\n","    y_pred0 = y_pred[:,:,data_idx]\n","    y_test_reshaped = result_reshape(y_test0)\n","    y_pred_reshaped = result_reshape(y_pred0)\n","\n","    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 8))\n","\n","    fig.suptitle('Active Constraints Distribution: ' + test_case.split('.')[0], size=19, y=0.88)\n","    axes[0].set_title('y_test', size=15, y=1.03)\n","    axes[1].set_title('y_pred', size=15, y=1.03)\n","    # axes[2].set_title('y_pred_binary', size=15, y=1.03)\n","\n","    sns.heatmap(y_test_reshaped,\n","                xticklabels=False,\n","                yticklabels=False,\n","                cbar_kws={'ticks': [-1, 0, 1], 'shrink': .75},\n","                square=True,\n","                ax=axes[0])\n","\n","    sns.heatmap(y_pred_reshaped,\n","                xticklabels=False,\n","                yticklabels=False,\n","                cbar_kws={'ticks': [-1, 0, 1], 'shrink': .75},\n","                square=True,\n","                ax=axes[1])\n","    \n","\n","    fig.show()\n","    # file_dir = path.join('drive/My Drive/OPF_Porject_EE394V_SPR2020-master/codes/experiments/figures/', test_case.split('.')[0] + '.png')\n","    # fig.savefig(file_dir, format='png')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wrfl_miqWLip","colab_type":"code","colab":{}},"source":["print(y_test1[:,:,1].transpose(0,1))\n","print(indices[:,:,1].transpose(0,1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LNlT68rcHGHV","colab_type":"code","colab":{}},"source":["# y_test1.resize_((n_bus+n_line,n_test))\n","# indices_int.resize_((n_bus+n_line,n_test))\n","# indices.resize_((n_bus+n_line,n_test))\n","\n","test_cases = [\n","    'pglib_opf_case24_ieee_rts.pickle', \n","]\n","\n","data_idx = [7,10,33,55,64,78,96]\n","data_idx = np.arange(10)\n","# test_vs_pred(y_test, y_pred, data_idx, test_cases[case_idx])\n","for i in range(len(data_idx)):\n","  test_vs_pred(y_test1, indices, data_idx[i], test_cases[0])"],"execution_count":null,"outputs":[]}]}